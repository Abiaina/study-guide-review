---
title: Observability
---

---

# ðŸ“˜ Observability

Observability is often broken into the **three pillars**: **Logs, Metrics, Traces**. A modern system usually combines all three.

---

## 1. Metrics

- **Prometheus** â†’ pull-based metrics collection. Best with Kubernetes.
- **Datadog** â†’ SaaS monitoring platform (agents on hosts, integrations).
- **CloudWatch Metrics** â†’ AWS-native; integrates with alarms.
- **Azure Monitor, GCP Monitoring** â†’ cloud-native equivalents.

---

## 2. Visualization

### Grafana

- **Purpose:** dashboards and visualization for time-series metrics.
- **Sources:** can use Prometheus, InfluxDB, Elasticsearch, CloudWatch, Loki, and more.
- **Key Concepts:**

  - **Dashboards**: panels (graphs, tables, gauges).
  - **Templating**: variables (e.g., `$cluster`, `$namespace`).
  - **Alerting**: integrated alerts since Grafana 8, can trigger Slack/PagerDuty/etc.
  - **Plugins**: extend to new data sources.

- **Best Practices:**

  - Organize dashboards per _team_ or _service_.
  - Show SLOs: latency (p95, p99), error rate, request throughput.
  - Keep dashboards simple â€” 6â€“8 panels max per view.
  - Use _provisioning_ (`dashboards/` directory with JSON + `grafana.ini`).

**Example: Prometheus query in Grafana panel**

```
rate(http_requests_total{job="api",status=~"5.."}[5m])
```

This would plot the 5xx error rate.

---

## 3. Logs

- **CloudWatch Logs** â†’ AWS log storage/queries.
- **Splunk** â†’ enterprise log aggregation/search.
- **ELK (Elasticsearch + Logstash + Kibana)** â†’ open-source stack.
- **Loki (Grafana Labs)** â†’ log aggregation, pairs with Prometheus.
- **New Relic Logs** â†’ SaaS, correlated with APM traces.

---

## 4. Traces

- **OpenTelemetry** â†’ vendor-neutral standard; instrument once, export anywhere.
- **Jaeger** â†’ CNCF tracing tool.
- **Zipkin** â†’ lightweight tracer.
- **Datadog APM** â†’ integrated metrics/logs/traces.
- **AWS X-Ray** â†’ request tracing in AWS stack.

---

## 5. Putting It Together

- **Metrics**: numeric signals â†’ detect anomalies.
- **Logs**: detailed context â†’ debug failures.
- **Traces**: request path â†’ diagnose latency & dependencies.

**Example Workflow**

1. Alert triggers: `p95 latency > 500ms` in Grafana.
2. Jump into logs (Splunk/Loki) for error details.
3. Trace with Jaeger/X-Ray to see which microservice caused the bottleneck.
